{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Distributed denial-of-service (DDoS)** attack is a form of attack that targets critical systems such as servers, websites or network resources by temporarily or permanently disrupting connection to the network host, causing a denial of service for users. A typical DDoS attack is accomplished by flooding the target with excess traffic to render the service offline. DDoS attacks are practically impossible to prevent and pose several information and business risks.\n",
    "\n",
    "**Binary Classification** is the task of classifying a dataset into two given groups. In Machine Learning, a binary classifier is a form of supervised learning, which means an input is mapped to an output based on example input-output pairs. For example, food would be an input and edible/inedible would be outputs. A binary classifier would attempt to predict edible or inedible food types by learning from examples of edible and inedible food. <br/>\n",
    "Some commonly used methods for binary classification include decision trees, random forests, bayesian networks, support vector machines, neural networks and logistic regression. Binary classifiers have a dependent variable with two possible values, such as pass/fail, win/loss or safe/unsafe and one or more independent variables which contain any real value.\n",
    "\n",
    "***\n",
    "\n",
    "## Outline\n",
    "\n",
    "In this example, we use a dataset that contains multiple types of DDoS attacks - Netbios, Portmap and Syn - which take place over two different days. The dataset will be split into two sets **training** and **testing**. The training set will be used to train a logistic regression model to predict our testing set by classifying DDOS or BENIGN connections based on the independent variables. The dependent variable is a label of two values NETBIOS/PORTMAP (DDoS attacks) or BENIGN (Harmless connection) and the independent variables are real network data such as flow duration, packets sent per second, lengths of forward packets and more. <br/>\n",
    "In simple terms, our model will look at the the day 1 data and learn what network data is labelled as NETBIOS/PORTMAP or BENIGN and attempt to predict whether connections from day 2 are DDoS connections or benign connections without looking at their correct labels. The predicted labels are compared to actual labels to obtain a classification report containing accuracy and error rates.\n",
    "\n",
    "\n",
    "### Contents\n",
    "- [Part 0: Getting Started](#part0)\n",
    "- [Part 1: Load The Data](#part1)\n",
    "- [Part 2: Clean The Data](#part2)\n",
    "- [Part 3: Explore The Data](#part3.0) <br/>\n",
    "  [3.1: Overview of data and features](#part3.1) <br/>\n",
    "  [3.2: Numerical and categorical features](#part3.2) <br/>\n",
    "- [Part 4: Machine Learning](#part4.0) <br/>\n",
    "  [4.1: Feature Selection](#part4.1) <br/>\n",
    "  [4.2: Training](#part4.2) <br/>\n",
    "  [4.3: Prediction](#part4.3) <br/>\n",
    "- [Part 5: Evaluation](#part5.0) <br/>\n",
    "  [5.1: Classification report](#part5.1) <br/>\n",
    "  [5.2: Confusion matrix](#part5.2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part0'></a>\n",
    "# Part 0: Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Formatting\n",
    "**Sci-kit learn (sklearn)** is a commonly used python machine learning library that features various classification, regression and clustering algorithms and powerful data pre and post processing and evaluation methods \n",
    "\n",
    "**Pandas** is a data processing and manipulation module used in python which can read various files and augment the data \n",
    "\n",
    "**Numpy** is a mathematical library that supports multi-dimensional arrays and matrices and high-level math functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas \n",
    "import numpy\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "from IPython.display import display\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "# Set display format of pandas output to \"%.3f\" (makes things look pretty)\n",
    "# Rounds any floating point decimal value to 3 places after decimal point  \n",
    "pandas.set_option('display.float_format', lambda x: '%.3f' % x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part1'></a>\n",
    "# Part 1: Load the Data\n",
    "\n",
    "To accomodate for long run-times for large datasets such as the one used in this example, we read only columns that are required. 20 preselected columns are used for this machine learning example. Add the following line into the **TODO** section of code cell below to read in required columns/features.\n",
    "\n",
    "```python\n",
    "use_columns = [\"Flow ID\", \"Source IP\", \"Source Port\", \"Destination IP\", \"Destination Port\", \"Timestamp\", \"Flow Duration\", \"Total Fwd Packets\", \"Total Backward Packets\", \"Fwd IAT Mean\", \"Fwd IAT Std\", \"Fwd IAT Min\", \"Fwd Packets/s\", \"Bwd Packets/s\",\"SYN Flag Count\", \"RST Flag Count\", \"PSH Flag Count\", \"ACK Flag Count\", \"URG Flag Count\", \"Label\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put column names into a list\n",
    "header_names = list(pandas.read_csv('netbios_day1.csv', nrows=0))\n",
    "\n",
    "# Remove leading and trailing whitespaces\n",
    "for i in range(0, len(header_names)):\n",
    "    header_names[i] = header_names[i].strip()\n",
    "\n",
    "# List of columns to use \n",
    "# TODO    \n",
    "use_columns = [\"Flow ID\", \"Source IP\", \"Source Port\", \"Destination IP\", \"Destination Port\", \"Timestamp\", \"Flow Duration\", \"Total Fwd Packets\", \"Total Backward Packets\", \"Fwd IAT Mean\", \"Fwd IAT Std\", \"Fwd IAT Min\", \"Fwd Packets/s\", \"Bwd Packets/s\",\"SYN Flag Count\", \"RST Flag Count\", \"PSH Flag Count\", \"ACK Flag Count\", \"URG Flag Count\", \"Label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data\n",
    "\n",
    "Pandas provides easy ways to read multiple file types. In our case, the dataset is a comma-seperated values (csv) file. An easy to use function to read csv files is\n",
    "\n",
    "```python\n",
    "#Usage: pandas.read_csv(filename, nrows, skiprows, names, usecols, ...)\n",
    "pandas.read_csv()\n",
    "```\n",
    "which takes in parameters like:<br/>\n",
    "\n",
    "- ```filename``` name of the file \n",
    "- ```nrows``` to specify number of rows to read\n",
    "- ```skiprows``` to skip any number of rows\n",
    "- ```names``` to specify header names\n",
    "- ```usecols``` to specify which column names to read, and more\n",
    "\n",
    "\n",
    "We read in 3 different files: \n",
    "\n",
    "- **'netbios_day1.csv'** contains training data from day 1\n",
    "- **'netbios_day2.csv'** contains netbios testing data from day 2\n",
    "- **'portmap.csv'** contains portmap testing data from day 2\n",
    "\n",
    "The following code reads data from 'netbios_day1.csv' and puts it into a pandas dataframe, which is a two-dimensional tabular data structure:\n",
    "\n",
    "```python\n",
    "dataframe_train = pd.read_csv('netbios_day1.csv', names = header_names, skiprows=1, usecols=use_columns)\n",
    "```\n",
    "\n",
    "Use the same template to read in the other two test data ('netbios_day2.csv' and 'Portmap.csv') in the **TODO** section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example read data into df_train from 'netbios_day1.csv'\n",
    "# Use header_names extracted previously\n",
    "# Skip first row which contains header names and use custom column names\n",
    "dataframe_train = pandas.read_csv('netbios_day1.csv', names = header_names, skiprows=1, usecols=use_columns)\n",
    "\n",
    "# TODO\n",
    "dataframe_test = pandas.read_csv('netbios_day2.csv', names = header_names, skiprows=1, usecols=use_columns)\n",
    "dataframe_portmap = pandas.read_csv('portmap.csv', names = header_names, skiprows=1, usecols=use_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part2'></a>\n",
    "# Part 2: Clean the Data\n",
    "\n",
    "Cleaning data is one of the key steps in data preprocessing. There are several values that we would like to remove from our data before running a machine learning algorithm on it. Data that is not clean could hurt insights and deem your results useless or lead to false conslusions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infinity\n",
    "\n",
    "Removing rows containing infinity is important as these could skew our results\n",
    "\n",
    "```python\n",
    "#Usage: dataframe.replace([values, to, replace], value)\n",
    "dataframe.replace()\n",
    "```\n",
    "\n",
    "can be used to replace all infinity values with null values which we will remove in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_train = dataframe_train.replace([numpy.inf, -numpy.inf], numpy.nan)\n",
    "dataframe_test = dataframe_test.replace([numpy.inf, -numpy.inf], numpy.nan)\n",
    "dataframe_portmap = dataframe_portmap.replace([numpy.inf, -numpy.inf], numpy.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Values\n",
    "\n",
    "Null values are values that are missing or not present in the database. These values are commonly indicated by a NaN. They can easily be removed with the function\n",
    "\n",
    "```python\n",
    "dataframe.dropna()```\n",
    "\n",
    "which drops all rows with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_train = dataframe_train.dropna()\n",
    "dataframe_test = dataframe_test.dropna()\n",
    "dataframe_portmap = dataframe_portmap.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Values\n",
    "\n",
    "We want to eliminate any columns with all zero values as these do not contribute to our algorithm and may increase run-time\n",
    "\n",
    "```python\n",
    "#Usage: dataframe.loc[label to look for or boolean array] \n",
    "dataframe.loc[]\n",
    "```\n",
    "\n",
    "\n",
    "```(dataframe != 0)``` returns a boolean dataframe indicating columns having non-zero entries  <br />\n",
    "```(dataframe != 0).any(axis=0)``` returns a boolean series indicating which columns have non-zero entries  <br />\n",
    "```df.loc``` can then be used to select those columns with non-zero entries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_train = dataframe_train.loc[:, (dataframe_train != 0).any(axis=0)]\n",
    "dataframe_test = dataframe_test.loc[:, (dataframe_test != 0).any(axis=0)]\n",
    "dataframe_portmap = dataframe_portmap.loc[:, (dataframe_portmap != 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part3.0'></a>\n",
    "# Part 3: Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part3.1'></a>\n",
    "### 3.1 Overview of data and features\n",
    "\n",
    "We explore our data to get an overview of our training set and to answer questions such as <br/>\n",
    "How many rows and columns does the dataset contain? <br/>\n",
    "What are the names of the features (columns)? <br/>\n",
    "Which features are identifying, numerical and categorical? <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "dataframe.shape```\n",
    "\n",
    "```python\n",
    "dataframe.head()```\n",
    "\n",
    "```python\n",
    "dataframe.info()```\n",
    "\n",
    "These three functions answers the questions above by printing the first 5 rows of the table, number of rows, columns and details about the features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape, head, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape prints out the number of rows and number of columns of the dataframe. The shape of a dataframe can be obtained by calling dataframe.shape\n",
    "\n",
    "Print the shape of the three dataframes in the **TODO** sections below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find shape of dataframe_train\n",
    "# TODO: Print training dataframe shape\n",
    "print()\n",
    "print(\"*\" * 50)\n",
    "\n",
    "# TODO: Print netbios test dataframe shape\n",
    "print()\n",
    "print(\"*\" * 50)\n",
    "\n",
    "#TODO: Print the portmap test dataframe shape\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head prints out the first five values of the dataframe. The head of a dataframe can be found by calling dataframe.head()\n",
    "\n",
    "Print the head of the dataframes in the **TODO** sections of the code cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data:\")\n",
    "dataframe_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Netbios testing data: \")\n",
    "# TODO: print head() of Netbios testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Portmap testing data: \")\n",
    "# TODO: print head() of portmap testing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Info provides the data types of our features. This information is important since only numerical data and Label is required for our machine learning analysis. Furthermore, we would like to leave out certain identifying features such as Source Port and Destination Port.\n",
    "\n",
    "Print out dataframe.info() in the **TODO** sections below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Training Data : \")\n",
    "# TODO: print info() of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain counts of values within each column with \n",
    "\n",
    "```python\n",
    "dataframe['column name'].value_counts()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data Label Counts:\")\n",
    "# TODO: print counts of 'Label' values of training data\n",
    "print(dataframe_train['Label'].value_counts())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Testing Data Label Counts:\")\n",
    "# TODO: print counts of 'Label' values of Netbios testing data\n",
    "print()\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Portmap Label Counts:\")\n",
    "# TODO: print counts of 'Label' values of Portmap testing data\n",
    "print()\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this exploratory analysis we can conclude that:\n",
    "\n",
    "**Training Data:**\n",
    "- Our training data has 19 columns \n",
    "- 12 numerical features, 6 categorical features and the label \n",
    "- 4,094,986 total entries or number of rows\n",
    "- 4,093,279 labels are DDOS attacks and 1707 labels are BENIGN.\n",
    "\n",
    "**Netbios Testing Data:**\n",
    "- Our netbios testing data also has 19 columns \n",
    "- 3,455,899 total entries or number of rows\n",
    "- 3,454,578 labels are NETBIOS attacks and 1321 are BENIGN.\n",
    "\n",
    "**Portmap Testing Data:**\n",
    "- Our portmap testing data also has 19 columns \n",
    "- 191,694 total entries or number of rows\n",
    "- 186,960 labels are PORTMAP attacks and 4734 are BENIGN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part3.2'></a>\n",
    "### 3.2 Numerical and Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can seperate our numerical and categorical features by searching for a specific data type in our dataframe. Categorical datatypes are characterized by being an \"object\". So, all columns with datatypes that are numerical are candidates for our training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_feats = dataframe_train.dtypes[dataframe_train.dtypes != \"object\"].index\n",
    "print(\"Number of Numerical features: \", len(numerical_feats))\n",
    "\n",
    "categorical_feats = dataframe_train.dtypes[dataframe_train.dtypes == \"object\"].index\n",
    "print(\"Number of Categorical features: \", len(categorical_feats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain numerical features such as Source Port and Destination Port are numerical however can be considered as identifying features and therefore should be left out from our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Numerical Features: \")\n",
    "print(dataframe_train[numerical_feats].columns)\n",
    "print(\"*\"*100)\n",
    "print(\"Categorical Features: \")\n",
    "print(dataframe_train[categorical_feats].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Create indicator variable from our Label feature. Indicator variable is the dependent variable or the variable that we attempt to predict. For ease of use we map our labels such that: <br/>\n",
    "NETBIOS/Portmap ---> 1 <br/>\n",
    "BENIGN          ---> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_train['Label'] = [1.0 if x == \"DrDoS_NetBIOS\" else 0.0 for x in dataframe_train['Label']]\n",
    "dataframe_test['Label'] = [1.0 if x == \"NetBIOS\" else 0.0 for x in dataframe_test['Label']]\n",
    "dataframe_portmap['Label'] = [1.0 if x == \"Portmap\" else 0.0 for x in dataframe_portmap['Label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part4.0'></a>\n",
    "# Part 4: Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part4.1'></a>\n",
    "### 4.1 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection is important to machine learning as it plays a crucial role in delivering accurate outputs. Irrelevant features can decrease accuracy of the models or even prevent algorithms from running to completion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation is a numerical measure of the relationship between two variables. Higher correelation means two variables have a strong relationship with each other. In our dataset, it would be beneficial to map the correlation between our numerical features against our Label in order to pick out the features with highest correlation.\n",
    "\n",
    "The code below generates a heatmap of correlation between each column of the dataframe.\n",
    "\n",
    "We need to look at the correlation of each value on the y-axis against the Label in the x-axis to find the correlation between our independent and dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrx = dataframe_train.corr()\n",
    "fig, ax = plt.subplots(figsize=(16,16))\n",
    "sns.heatmap(corr_matrx, annot=True, ax=ax, fmt='0.3f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting features\n",
    "\n",
    "```python\n",
    "# Usage dataframe.loc[ [columns to select] ]\n",
    "dataframe.loc[]\n",
    "```\n",
    "\n",
    "can be used to select subset of columns or rows from our dataframe.\n",
    "\n",
    "we can create a dataframe with our required features by including a list of the required features within the dataframe.loc[] function call. Uncomment the code in the code cell and copy the list below that contains the numerical features and paste it into the dataframe.loc[] function call. The list of features can be manually changed, such as excluding features with low correlation, to test for different accuracies.\n",
    "\n",
    "``` python\n",
    "[\"Flow Duration\", \"Total Fwd Packets\", \"Total Backward Packets\", \"Fwd IAT Mean\", \"Fwd IAT Std\", \"Fwd IAT Min\", \"Fwd Packets/s\", \"Bwd Packets/s\",\"SYN Flag Count\", \"RST Flag Count\", \"ACK Flag Count\", \"URG Flag Count\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Include a list of required features within dataframe_train[]\n",
    "dataframe_features = dataframe_train[ [\"Flow Duration\", \"Total Fwd Packets\", \"Total Backward Packets\", \"Fwd IAT Mean\", \"Fwd IAT Std\", \"Fwd IAT Min\", \"Fwd Packets/s\", \"Bwd Packets/s\",\"SYN Flag Count\", \"RST Flag Count\", \"ACK Flag Count\", \"URG Flag Count\"] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set up two arrays that contain the dependent and independent variables. Our dependent variable is the Label, i.e, NETBIOS/PORTMAP (1) or BENIGN (0),  and our independent variables are the features we selected in the code cells above.  \n",
    "\n",
    "Copy the following code into the code cell below and run it to set up Y_train which stores the label values in an array and X_train which stores the numerical features in an array\n",
    "\n",
    "```python\n",
    "Y_train = dataframe_train['Label'].values\n",
    "X_train = dataframe_features.values\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create X_train and Y_train variables \n",
    "Y_train = dataframe_train['Label'].values\n",
    "X_train = dataframe_features.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part4.2'></a>\n",
    "### 4.2 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training in machine learning involves a sequence of setting up dependent and independent variables, running an algorithm on the data and building a mathematical model based on the sample data.\n",
    "\n",
    "We set up our required variables in the previous section. Now we choose our classifier to build our model. In this example we use the binary logistic regression model. Binary logistic regression is frequently used when our dependent variable is categorical and has only two possible outcomes, in our case, ddos or benign. \n",
    "\n",
    "Our X_train and Y_train is then passed into our machine learning algorithm, in this case logistic regression, to build our model. Copy the following code into the code cell below to build the model.\n",
    "\n",
    "```python\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train, Y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create logistic regression model\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part4.3'></a>\n",
    "### 4.3 Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use our trained logistic regression model to see how well it can predict if our testing data is a DDOS attack or benign.\n",
    "\n",
    "Once again, we split our testing data into X_test and Y_test. X_test contains our numerical features which we will use to predict new labels. Y_test contains the correct labels that will then be compared against the labels predicted using X_test and the logistic regression model. \n",
    "\n",
    "The following code can be copied into the code cell and ran to create the variables and predictions for the netbios data.\n",
    "\n",
    "```python\n",
    "Y_test = dataframe_test['Label'].values\n",
    "X_test = dataframe_test.drop(categorical_feats, axis=1)\n",
    "X_test = X_test.drop(['Source Port', 'Destination Port'], axis=1).values\n",
    "\n",
    "predictions = logmodel.predict(X_test)\n",
    "```\n",
    "\n",
    "The following code creates the variables and predictions for portmap data:\n",
    "\n",
    "```python\n",
    "Y_test_portmap = dataframe_test['Label'].values\n",
    "X_test_portmap = dataframe_test.drop(categorical_feats, axis=1)\n",
    "X_test_portmap = X_test_portmap.drop(['Source Port', 'Destination Port'], axis=1).values\n",
    "predictions_portmap = logmodel.predict(X_test_portmap)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = dataframe_test['Label'].values\n",
    "X_test = dataframe_test.drop(categorical_feats, axis=1)\n",
    "X_test = X_test.drop(['Source Port', 'Destination Port'], axis=1).values\n",
    "predictions = logmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_portmap = dataframe_portmap['Label'].values\n",
    "X_test_portmap = dataframe_portmap.drop(categorical_feats, axis=1)\n",
    "X_test_portmap = X_test_portmap.drop(['Source Port', 'Destination Port'], axis=1).values\n",
    "predictions_portmap = logmodel.predict(X_test_portmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part5.0'></a>\n",
    "# Part 5: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have completed the machine learning, we can evaluate how well our logistic regression model has performed against the testing data. We use two reports to find out our accuracy - classification report and confusion matrix. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part5.1'></a>\n",
    "### 5.1 Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classification report displays the precision, recall, F1, and support scores for the model.\n",
    "\n",
    "- **Precision:** Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. In our case, precision answers the question: Of all the network data that we labelled as NETBIOS/PORTMAP, how many were actually NETBIOS/PORTMAP\n",
    "- **Recall:** Recall or sensitivity is the ratio of correctly predicted positive observations to all observations in the actual class. Recall answers the question: Of all the network data that truly is NETBIOS/PORTMAP, how many did we label?\n",
    "- **F1 Score:** F1 score is the weighted average of precision and recall\n",
    "- **Support:** Number of samples of the true dataset in each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 Netbios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this classification report 90% of BENIGN (0) labels were predicted correctly and 100% of NETBIOS DDOS (1) labels were predicted correctly. The recall and f1 score of BENIGN values are 22% and 35% respectively. \n",
    "\n",
    "For our BENIGN labels, the classifier has high precision and low recall which means the classifier is very selective. A majority of the results returned by the classifier are correctly predicted as BENIGN attacks, however the classifier misses a lot of BENIGN attacks because it is very picky. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 Portmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test_portmap,predictions_portmap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "91% of BENIGN (0) labels were predicted correctly and 98% of PORTMAP (1) labels were predicted correctly. BENIGN had a recall of 24% and F1 score of 37% which means \n",
    "\n",
    "Similarily for the portmap testing the classifier has a high precision but low recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part5.2'></a>\n",
    "## 5.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand our precision, recall and F1 scores we create a confusion matrix. The confusion matrix shows the actual and predicted labels from a classifier. The output is a grid that contains 4 values - True Positive (top left), False Positive (top right), False Negatives (bottom left) and True Negatives (bottom right)\n",
    "\n",
    "**True Positive** refers to number of labels that are positive (1) and correctly predicted as positive (1)\n",
    "\n",
    "**False Positive** refers to number of labels that are actually negative (0) but incorrectly predicted as positive (1)\n",
    "\n",
    "**False Negative** refers to number of labels that are actually positive (1) but incorrectly predicted as negative (0)\n",
    "\n",
    "**True Negative** refers to number of labels that are negative (0) and correctly predicted as negative (0)\n",
    "\n",
    "The percent of true positive and true negative scores together make up the accuracy of the model and the percent of false positives and false negatives together make up the error or misclassification rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 Netbios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(Y_test,predictions)\n",
    "print(cm)\n",
    "\n",
    "accuracy = (cm[0,0]+cm[1,1])/cm.sum()*100\n",
    "error = (cm[0,1]+cm[1,0])/cm.sum()*100\n",
    "\n",
    "print('Accuracy of the model: {0:.2f}%'.format(accuracy))\n",
    "print('Error/ Misclassification rate: {0:.2f}%'.format(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the grid we can identify that our scores are as follows:\n",
    "- True Positive: 287\n",
    "- False Positive: 1034\n",
    "- False Negative: 33\n",
    "- True Negative: 3454545\n",
    "\n",
    "Our calculated accuracy is 99.97% and error rate is 0.03%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 Portmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(Y_test_portmap,predictions_portmap)\n",
    "print (cm)\n",
    "accuracy = (cm[0,0]+cm[1,1])/cm.sum()*100\n",
    "error = (cm[0,1]+cm[1,0])/cm.sum()*100\n",
    "\n",
    "\n",
    "print('Accuracy of the model: {0:.2f}%'.format(accuracy))\n",
    "print('Error/ Misclassification rate: {0:.2f}%'.format(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the grid we can identify that our scores are as follows:\n",
    "- True Positive: 1113\n",
    "- False Positive: 3621\n",
    "- False Negative: 107\n",
    "- True Negative: 186853\n",
    "\n",
    "Our calculated accuracy is 98.06% and error rate is 1.94%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "\n",
    "- Pandas Documentation: https://pandas.pydata.org/docs/\n",
    "- Sci-kit learn Docmentation https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "- Seaborn Documentation: https://seaborn.pydata.org/introduction.html\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
